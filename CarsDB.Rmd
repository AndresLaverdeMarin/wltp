---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Parse excel files into a HDF5 file
It builds this tree inside a [HDF5 file](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-hdf5):
```
cars/
    +--inp/
    |   +--specs              (df) test-cars as delivered by Heinz on 13 May 2019 
    |   +--pwots              (df) full-load-curves for these cars
    |
    +--cooked/                preprocessed cars for python consumptio.
        +--veh001/
        |   +--spec           (df) all kv-pairs from specs + SM, ASM
        |   +--pwot           (df) a single-column df(wot) indexed by n
        |   +--out1           (df) results produced by MSAccess
        |   +--out2           (df) results produced by python
        |
        +--veh.../
```

## Instructions, to run this notebook in your own jupyter-server
...read the `README.md`.

```{python}
import functools as ftt
import itertools as itt
import logging
from pathlib import Path
import re
from typing import Tuple, Dict

import numpy as np
from pandalone import xleash
import qgrid
import pandas as pd
from pandas import HDFStore

# Add %%black at the top of a cell, and re-evaluate it, 
# to format it before git-commits, and ease diffs.
# %load_ext blackcellmagic

idx = pd.IndexSlice
log = logging.getLogger('CarsDB.ipynb')
```

```{python}
## DEFINITIONS
#
h5fname = 'WltpCars.h5'
# Test cars delivered by Heinz on 13 May 2019
heinzdb_fpath="WLTP_GS_calculation_15032019_for_prog_code_subgroup.accdb"
xlfname = Path("example vehicles for the prog code validation.xlsx")
veh_group_fmt = "/cars/cooked/veh%0.3d"
c_vehnum, c_n, c_pwot, c_SM, c_ASM = 'vehicle_no', 'n', 'Pwot', 'SM', 'ASM'
```

```{python}
from pandas import HDFStore

def openh5(h5fname):
    return HDFStore(h5fname, 
                    encoding='utf-8', 
                    # Not the strongest one, *repack* it before git-commit.
                    complevel=6, complib='blosc:blosclz')
```

```{python}
## UNCOMMENT next command & run to DELETE the db-file, and rebuild it.
# #!rm {h5fname}
```

```{python}
def print_nodes(h5fname=h5fname):
    from columnize import columnize

    ## Print head & taiul of groups in h5db.
    #
    with openh5(h5fname) as h5db:
        nodes = h5db.keys()
        print(columnize(nodes, displaywidth=160))
print_nodes()
```

```{python}
from pandalone import xleash

specs = xleash.lasso('%s#technical data!::["df"]' % xlfname)
pwots = xleash.lasso('%s#wot power curves!::["df"]' % xlfname)
```

```{python}
import io, json, time, platform
from datetime import datetime
from ruamel.yaml import YAML
from copy import deepcopy


def _human_time(unixtime: int = None) -> str:
    if unixtime is None:
        tm = datetime.now()
    else:
        tm = datetime.utcfromtimestamp(unixtime)
    return tm.strftime("%Y-%m-%d %H:%M:%S")


def _file_hashed(fpath, algo="md5") -> Tuple[str, str]:
    import hashlib
    import io

    digester = hashlib.new(algo)
    with open(fpath, "rb") as f:
        for b in iter(lambda: f.read(io.DEFAULT_BUFFER_SIZE), b""):
            digester.update(b)
    return algo, digester.hexdigest()


# file_hashed(xlfname)


def _provenir_fpath(fpath, algos=("md5", "sha256")) -> Dict[str, str]:
    s = {"fpath": str(fpath)}

    fpath = Path(fpath)
    if fpath.exists():
        s["hexsums"] = dict(_file_hashed(fpath, algo) for algo in algos)
        s["ctime"] = _human_time(fpath.stat().st_ctime)

    return s


def _git_describe():
    import subprocess as sbp

    try:
        return sbp.check_output("git describe --always".split()).trim()
    except Exception as ex:
        log.info("Cannot git-describe due to: %s", ex)


def provenance_info(*fpaths, prov_info=None) -> Dict[str, str]:
    """
    :param prov_info:
        if given, reused (cloned first), and any fpaths appended in it.
    """
    if prov_info:
        info = deepcopy(prov_info)

        fps = info.get("fpaths")
        if not isinstance(fpaths, list):
            fps = info["fpaths"] = []
        fps.extend(_provenir_fpath(f) for f in fpaths)
    else:
        info = {"ctime": _human_time(), "uname": dict(platform.uname()._asdict())}
        
        gitver = _git_describe()
        if gitver:
            info["gitver"] = gitver

        if fpaths:
            info["fpaths"] = [_provenir_fpath(f) for f in fpaths]

    return info


#prov_info = _provenance_info(xlfname)
#prov_info = provenance_info('sfdsfd', prov_info=prov_info)
#provenance_info('ggg', prov_info=prov_info)


def yaml_dumps(o) -> str:
    s = io.StringIO()
    yaml = YAML(typ="safe", pure=True)
    yaml.default_flow_style = False
    # yaml.canonical = True
    yaml.dump(prov_info, s)
    return s.getvalue()


def provenir_hdf_node(h5db, node, title=None, *fpaths, prov_info=None):
    h5file = h5db._handle
    if title:
        h5file.set_node_attr(node, "TITLE", title)
    if prov_info is None:
        prov_info = provenance_info(*fpaths)

    provenance = yaml_dumps(prov_info)
    h5file.set_node_attr(node, "provenance", provenance)

```

```{python}
import qgrid

display(qgrid.show_grid(specs))
display(qgrid.show_grid(pwots))
```

```{python}
prov_info = provenance_info(heinzdb_fpath, xlfname)

with openh5(h5fname) as h5db:
    h5db.put("/cars/input/specs", specs)
    provenir_hdf_node(
        h5db,
        "/cars/input/specs",
        "Specs for 114 test-cars, as delivered by Heinz on 13 May 2019",
        prov_info=prov_info,
    )
    h5db.put("/cars/input/pwots", pwots)
    provenir_hdf_node(
        h5db,
        "/cars/input/pwots",
        "Full-load-curves for the 114 test-cars, as delivered by Heinz on 13 May 2019",
        prov_info=prov_info,
    )
```

```{python}
def extract_SM_from_pwot(
    pwot, c_n=c_n, c_pwot=c_pwot, c_SM=c_SM, c_ASM=c_ASM
) -> "Tuple(pd.DataFrame, float)":
    """
    Keep just (n, Pwot, ASM) columns & extract SM column as scalar value.
    
    :param pwot:
        the wot-curve for a single vehicle, a df like::
        
            IX	vehicle_no	n	Pwot	SM	ASM
            -----------------------
            0	1			800	5.43	0.1	0.0
            0	1			900	9.01	0.1	0.0
    :return:
        wot(n: pwot, ASM), SM
        where `wot` is indexed by engine-speed(n)
    """
    pwot2 = pwot.loc[:, [c_n, c_pwot, c_ASM]].set_index(c_n)
    row1 = pwot.iloc[0, :]

    SM = row1[c_SM]
    assert (pwot[c_SM] == SM).all(), pwot

    return pwot2, SM


# ## TEST
# extract_SM_from_pwot(pwots.loc[pwots[c_vehnum] == 3])
```

```{python}
prov_info = provenance_info(heinzdb_fpath, xlfname)


def store_group_per_car(
    h5db, specs, c_vehnum=c_vehnum, c_SM=c_SM, c_ASM=c_ASM, veh_group_fmt=veh_group_fmt
):
    """
    Distribute vehicle-specs each on a single HDF5 node with 2 children::
    
        cars/cooked/
                +--veh001/
                |   +--spec = all kv-pairs from matrix + SM, ASM
                |   +--pwot = a single-column df(wot) indexed by n
                +...
    """
    for _, spec in specs.iterrows():
        vehnum = spec[c_vehnum]
        vgroup = veh_group_fmt % vehnum
        spec_node = "%s/spec" % vgroup
        pwot_node = "%s/pwot" % vgroup

        pwot = pwots.loc[pwots[c_vehnum] == vehnum, :]
        pwot, SM = extract_SM_from_pwot(pwot)

        spec[c_SM] = SM

        h5db.put(spec_node, spec)
        h5db.put(pwot_node, pwot)

        provenir_hdf_node(
            h5db,
            spec_node,
            "Specs of the test-car, as delivered by Heinz on 13 May 2019",
            prov_info=prov_info,
        )
        provenir_hdf_node(
            h5db,
            pwot_node,
            "Full-load-curve of the test-car, as delivered by Heinz on 13 May 2019",
            prov_info=prov_info,
        )


with openh5(h5fname) as h5db:
    store_group_per_car(h5db, specs)
    provenir_hdf_node(
        h5db, "/cars/cooked", title="preprocessed cars for python consumption"
    )
```

```{python}
def fpath_to_vehnum(outfpath: Path) -> int:
    import re

    return int(re.match(r"V(\d+)\.xls", outfpath.name).group(1))


prov_info = provenance_info(heinzdb_fpath)


def store_results_per_car(
    h5db, results_dir="VehResults", veh_group_fmt=veh_group_fmt, out_group="out1"
):
    """
    Populate h5db with results collected from a folder full of (`V123.xls`, ...) exchel-files.
    """

    # A dir with one Excel-file per vehicle, as produced by MSAccess.
    results_dir = Path(results_dir)
    root_prov_info = provenance_info(heinzdb_fpath)
    for outfpath in results_dir.glob("*.xls"):
        vehnum = fpath_to_vehnum(outfpath)
        vgroup = veh_group_fmt % vehnum
        vgroup = "%s/%s" % (vgroup, out_group)
        outdf = xleash.lasso('%s#::["df"]' % outfpath)
        h5db.put(vgroup, outdf)
        provenir_hdf_node(
            h5db, vgroup, outfpath, prov_info=prov_info
        )


with openh5(h5fname) as h5db:
    store_results_per_car(h5db)
```

```{python}
print_nodes()
```

```{python}
# %%time
## COMPRESS x100 HDF5: 122MB --> ~3.1MB in 5.6s on my speedy laptop.
#
# !ls -lh WltpCars*
# !ptrepack WltpCars.h5 --complevel=9 --complib=blosc:blosclz -o WltpCars.1.h5
# !mv WltpCars.1.h5 WltpCars.h5
# !ls -lh WltpCars*
```

```{python}
with openh5(h5fname) as h5db:
    df = h5db.get('/cars/cooked/veh014/out1')
```

```{python}
df.columns
```
