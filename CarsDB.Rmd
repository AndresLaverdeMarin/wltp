---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

# Parse excel files into a HDF5 file
It builds this tree inside a [HDF5 file](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-hdf5):
```
vehicles/
    +--v001/
    |   +--props          (df) all kv-pairs from input/specs + SM, ASM
    |   +--pwot           (df) a single-column df(wot) indexed by n
    |   +--out1
    |   |   +--cycle      (df) cycle-run generated by HeinzDb
    |   |   +--props      (series) scalar inputs & outputs generated by HeinzDb
    |   |
    |   +--out2           (df) results generated by Python
    |       +--cycle      (df) cycle-run generated by Python 
    |       +--props      (series) scalar outputs generated by Python
    |
    +--v002/
        ...
```

## to run this notebook in your own jupyter-server
...read instructions, on the `README.md`,
## to inspect and get help on the HDF5 file
...read instructions, on the `README.md`,

```{python}
import functools as ftt
import itertools as itt
import logging
from pathlib import Path
import re
from typing import Tuple, Dict

import numpy as np
from pandalone import xleash
import qgrid
import pandas as pd
from pandas import HDFStore

# Add %%black at the top of a cell, and re-evaluate it, 
# to format it before git-commits, and ease diffs.
# %load_ext blackcellmagic

idx = pd.IndexSlice
log = logging.getLogger('CarsDB.ipynb')
```

```{python}
## DEFINITIONS
#
h5fname = 'WltpCars.h5'
# Test cars delivered by Heinz on 13 May 2019
heinzdb_fpath="WLTP_GS_calculation_15032019_for_prog_code_subgroup.accdb"
xlfname = Path("example vehicles for the prog code validation.xlsx")
vehs_node = '/Vehicles'
veh_group_fmt = vehs_node + "/v%0.3d"
c_vehnum, c_n, c_pwot, c_SM, c_ASM = 'vehicle_no', 'n', 'Pwot', 'SM', 'ASM'
```

```{python}
from pandas import HDFStore

def openh5(h5fname):
    return HDFStore(h5fname, 
                    encoding='utf-8', 
                    # Not the strongest one, *repack* it before git-commit.
                    complevel=6, complib='blosc:blosclz')
```

```{python}
## UNCOMMENT next command & run to DELETE the db-file, and rebuild it.
# !rm {h5fname}
```

```{python}
def print_nodes(h5fname=h5fname):
    from columnize import columnize

    ## Print head & taiul of groups in h5db.
    #
    with openh5(h5fname) as h5db:
        nodes = h5db.keys()
        print(columnize(nodes, displaywidth=160))
print_nodes()
```

```{python}
from pandalone import xleash

specs = xleash.lasso('%s#technical data!::["df"]' % xlfname)
pwots = xleash.lasso('%s#wot power curves!::["df"]' % xlfname)
```

```{python}
import io, json, time, platform
from datetime import datetime
from ruamel.yaml import YAML
from copy import deepcopy


def _human_time(unixtime: int = None) -> str:
    if unixtime is None:
        tm = datetime.now()
    else:
        tm = datetime.utcfromtimestamp(unixtime)
    return tm.strftime("%Y-%m-%d %H:%M:%S")


def _file_hashed(fpath, algo="md5") -> Tuple[str, str]:
    import hashlib
    import io

    digester = hashlib.new(algo)
    with open(fpath, "rb") as f:
        for b in iter(lambda: f.read(io.DEFAULT_BUFFER_SIZE), b""):
            digester.update(b)
    return algo, digester.hexdigest()


# file_hashed(xlfname)


def _provenir_fpath(fpath, algos=("md5", "sha256")) -> Dict[str, str]:
    s = {"fpath": str(fpath)}

    fpath = Path(fpath)
    if fpath.exists():
        s["hexsums"] = dict(_file_hashed(fpath, algo) for algo in algos)
        s["ctime"] = _human_time(fpath.stat().st_ctime)

    return s


def _git_describe():
    import subprocess as sbp

    try:
        return sbp.check_output("git describe --always".split()).trim()
    except Exception as ex:
        log.info("Cannot git-describe due to: %s", ex)


def provenance_info(*fpaths, prov_info=None) -> Dict[str, str]:
    """
    :param prov_info:
        if given, reused (cloned first), and any fpaths appended in it.
    """
    if prov_info:
        info = deepcopy(prov_info)

        fps = info.get("fpaths")
        if not isinstance(fpaths, list):
            fps = info["fpaths"] = []
        fps.extend(_provenir_fpath(f) for f in fpaths)
    else:
        info = {"ctime": _human_time(), "uname": dict(platform.uname()._asdict())}
        
        gitver = _git_describe()
        if gitver:
            info["gitver"] = gitver

        if fpaths:
            info["fpaths"] = [_provenir_fpath(f) for f in fpaths]

    return info


#prov_info = _provenance_info(xlfname)
#prov_info = provenance_info('sfdsfd', prov_info=prov_info)
#provenance_info('ggg', prov_info=prov_info)


def yaml_dumps(o) -> str:
    s = io.StringIO()
    yaml = YAML(typ="safe", pure=True)
    yaml.default_flow_style = False
    # yaml.canonical = True
    yaml.dump(prov_info, s)
    return s.getvalue()


def provenir_h5node(h5db, node, *fpaths, title=None, prov_info=None):
    h5file = h5db._handle
    if title:
        h5file.set_node_attr(node, "TITLE", title)
    if prov_info is None:
        prov_info = provenance_info(*fpaths)

    provenance = yaml_dumps(prov_info)
    h5file.set_node_attr(node, "provenance", provenance)

```

```{python}
import qgrid

display(qgrid.show_grid(specs))
display(qgrid.show_grid(pwots))
```

```{python}
def get_scalar_column(df, column):
    """
    Extract the single scalar value if column contains it.
    """
    col = df[column]
    val = col.iloc[0]
    
    assert col.isnull().all() or (col == val).all(), (column, df)
    
    return val


def drop_scalar_columns(df, scalar_columns) -> Tuple[pd.DataFrame, dict]:
    values = [get_scalar_column(df, c) for c in scalar_columns]
    scalars = dict(zip(scalar_columns, values))
    return df.drop(scalar_columns, axis=1), scalars
```

```{python}
def extract_SM_from_pwot(
    pwot, c_n=c_n, c_pwot=c_pwot, c_SM=c_SM, c_ASM=c_ASM
) -> "Tuple(pd.DataFrame, float)":
    """
    Keep just (n, Pwot, ASM) columns & extract SM column as scalar value.
    
    :param pwot:
        the wot-curve for a single vehicle, a df like::
        
            IX	vehicle_no	n	Pwot	SM	ASM
            -----------------------
            0	1			800	5.43	0.1	0.0
            0	1			900	9.01	0.1	0.0
    :return:
        wot(n: pwot, ASM), SM
        where `wot` is indexed by engine-speed(n)
    """
    SM = get_scalar_column(pwot, c_SM)
    pwot = pwot.loc[:, [c_n, c_pwot, c_ASM]].set_index(c_n)

    return pwot, SM


# ## TEST
# extract_SM_from_pwot(pwots.loc[pwots[c_vehnum] == 3])
```

```{python}
## Store each a vehicle into a node with 2 nodes: props, pwot
#
prov_info = provenance_info(heinzdb_fpath, xlfname)

def store_group_per_car(
    h5db, specs, c_vehnum=c_vehnum, c_SM=c_SM, c_ASM=c_ASM, veh_group_fmt=veh_group_fmt
):
    for _, spec in specs.iterrows():
        vehnum = spec[c_vehnum]
        vgroup = veh_group_fmt % vehnum
        spec_node = "%s/props" % vgroup
        pwot_node = "%s/pwot" % vgroup

        pwot = pwots.loc[pwots[c_vehnum] == vehnum, :]
        pwot, SM = extract_SM_from_pwot(pwot)

        spec[c_SM] = SM

        h5db.put(spec_node, spec)
        h5db.put(pwot_node, pwot)

        provenir_h5node(
            h5db,
            spec_node,
            title="Specs of the test-car, as delivered by Heinz on 13 May 2019",
            prov_info=prov_info,
        )
        provenir_h5node(
            h5db,
            pwot_node,
            title="Full-load-curve of the test-car, as delivered by Heinz on 13 May 2019",
            prov_info=prov_info,
        )


with openh5(h5fname) as h5db:
    store_group_per_car(h5db, specs)
    provenir_h5node(
        h5db, vehs_node, title="preprocessed cars for python consumption"
    )
```

```{python}
def fpath_to_vehnum(outfpath: Path) -> int:
    "Parse the folder-path containing the HeinzDb result excel files"
    import re

    return int(re.match(r"V(\d+)\.xls", outfpath.name).group(1))


prov_info = provenance_info(heinzdb_fpath)
scalar_columns = [
    "Calculation date",
    "Time at calculation start",
    "Version",
    "cycle_type",
    "cycle",
    "cycle_chosen_by_user",
    "vehicle_no",
    "vehicle",
    "vehicle_class",
    "kerb_mass in kg",
    "test_mass in kg",
    "rated power in kW",
    "rated_speed in min-1",
    "idling_speed in min-1",
    "n_min_drive_set",
    "n_min_drive_up",
    "n_min_drive_up_modified",
    "n_min_drive_down",
    "n_min_drive_down_modified",
    "n_min_drive_start_up",
    "n_min_drive_start_down",
    "t_end_start_phase",
    "n_min_drive_start_applied",
    "no_of_gears",
    "gear_v_max",
    "v_max_declared",
    "v_max_calculated",
    "f0",
    "f1",
    "f2",
    "speed cap in km/h",
    "p_downscale",
    "downscaling",
    "suppress gear 0 during downshifts",
    "n_max1",
    "n_max2",
    "n95_low",
    "n95_high",
    "average_gear",
]


def store_results_per_car(
    h5db,
    results_dir="VehResults",
    veh_group_fmt=veh_group_fmt,
    cycle_group_suffix="out1/cycle",
    props_group_suffix="out1/props",
):
    """
    Populate h5db with results collected from a folder full of (`V123.xls`, ...) exchel-files.
    """

    # A dir with one Excel-file per vehicle, as produced by MSAccess.
    results_dir = Path(results_dir)
    root_prov_info = provenance_info(heinzdb_fpath)
    for outfpath in results_dir.glob("*.xls"):
        vehnum = fpath_to_vehnum(outfpath)
        vgroup = veh_group_fmt % vehnum

        outdf = xleash.lasso('%s#::["df"]' % outfpath)
        outdf, props = drop_scalar_columns(outdf, scalar_columns)

        cycle_group = "%s/%s" % (vgroup, cycle_group_suffix)
        h5db.put(cycle_group, outdf)
        provenir_h5node(
            h5db,
            cycle_group,
            outfpath,
            title="Cycle-run produced by Heinz's MSAccess db",
            prov_info=prov_info,
        )

        props_group = "%s/%s" % (vgroup, props_group_suffix)
        h5db.put(props_group, pd.Series(props))
        provenir_h5node(
            h5db,
            props_group,
            outfpath,
            title="Properties (in & out) produced by Heinz's MSAccess db",
            prov_info=prov_info,
        )


with openh5(h5fname) as h5db:
    store_results_per_car(h5db)
```

```{python}
print_nodes()
```

```{python}
# %%time
## COMPRESS x100 HDF5: 122MB --> ~3.1MB in 5.6s on my speedy laptop.
#
# !ls -lh WltpCars*
# !ptrepack WltpCars.h5 --complevel=9 --complib=blosc:blosclz -o WltpCars.1.h5
# !mv WltpCars.1.h5 WltpCars.h5
# !ls -lh WltpCars*
```

```{python}
with openh5(h5fname) as h5db:
    outdf = h5db.get('/vehicles/v014/out1/cycle')
    inpsr = h5db.get('/vehicles/v014/props')
    outsr = h5db.get('/vehicles/v014/out1/props')
```

```{python}
qgrid.show_grid(outdf, grid_options={'forceFitColumns': False})
```

```{python}
display(inpsr, outsr, outdf.columns)
```
